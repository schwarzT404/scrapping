# Web Scraping Python - Exercices AvancÃ©s

Collection complÃ¨te de 9 exercices progressifs implÃ©mentant les techniques avancÃ©es de web scraping avec BeautifulSoup.

## ğŸ¯ Objectifs

- MaÃ®triser BeautifulSoup et requests pour l'extraction de donnÃ©es web
- ImplÃ©menter des patterns robustes (retry, cache, logging)
- GÃ©rer l'authentification et les sessions
- Nettoyer et valider les donnÃ©es extraites
- CrÃ©er des architectures modulaires et rÃ©utilisables

## ğŸ“¦ Installation

```bash
# Cloner le dÃ©pÃ´t
git clone https://github.com/schwarzT404/scrapping.git
cd scrapping

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸš€ Utilisation

### Menu interactif

```bash
python main_scraping_exercises.py
```

### Exercices individuels

```bash
python exercice_01_books_scraper.py
python exercice_02_quotes_graph.py
python exercice_03_fake_jobs.py
# ... etc
```

## ğŸ“‹ Liste des exercices

| # | Exercice | Description | DurÃ©e |
|---|----------|-------------|-------|
| 1 | **Books to Scrape** | Extraction complÃ¨te avec pagination (20 pages) | 3-4 min |
| 2 | **Quotes Graph** | Graphe relationnel avec cache (2 pages) | 30-45s |
| 3 | **Fake Jobs** | Filtres avancÃ©s et dÃ©tection doublons | 10-15s |
| 4 | **Market Analysis** | Statistiques et visualisations matplotlib | 45-60s |
| 5 | **Category Navigation** | Arborescence catÃ©gories complÃ¨te | 3-5 min |
| 6 | **Resilient Scraper** | Retry intelligent, logging, checkpoints | 2-3 min |
| 7 | **Data Cleaning** | Pipeline nettoyage et mÃ©triques qualitÃ© | 45-60s |
| 8 | **Multi-Source** | Architecture modulaire multi-sources | 1-2 min |
| 9 | **Authentication** | Gestion sessions et contenu protÃ©gÃ© | 20-30s |

## ğŸ“ Concepts couverts

### Techniques de base
- SÃ©lecteurs CSS et XPath
- Navigation dans le DOM
- Extraction de donnÃ©es structurÃ©es
- Gestion des erreurs

### Patterns avancÃ©s
- Pagination automatique
- SystÃ¨me de cache
- Retry avec backoff exponentiel
- Rate limiting adaptatif
- Checkpoints et reprise

### Architecture
- Design patterns (Strategy, Plugin)
- Parallel processing (ThreadPoolExecutor)
- Configuration externe (YAML)
- Logging structurÃ©

### Data Engineering
- Nettoyage et standardisation
- DÃ©tection d'anomalies
- Validation croisÃ©e
- MÃ©triques qualitÃ©

## ğŸ“Š Sorties gÃ©nÃ©rÃ©es

Les exercices gÃ©nÃ¨rent diffÃ©rents formats de sortie dans `./outputs/`:
- **JSON** : DonnÃ©es structurÃ©es avec mÃ©tadonnÃ©es
- **CSV** : Export tabulaire (offres d'emploi, donnÃ©es nettoyÃ©es)
- **GraphML** : Graphes relationnels (importable dans Gephi)
- **PNG** : Visualisations matplotlib
- **LOG** : Journaux dÃ©taillÃ©s d'exÃ©cution

## âš™ï¸ Configuration

### Limites par dÃ©faut

Toutes les limites sont configurables pour optimiser le temps d'exÃ©cution:

```python
# Exercice 1
scraper = BooksScraperComplete(max_pages=20)  # DÃ©faut: 20 pages

# Exercice 2
scraper = QuotesGraphScraper(max_pages=2)  # DÃ©faut: 2 pages

# Exercice 3
scraper = FakeJobsScraper(max_jobs=100)  # DÃ©faut: 100 offres
```

### Variables d'environnement

```bash
# Optionnel: timeout personnalisÃ©
export SCRAPER_TIMEOUT=15

# Optionnel: dÃ©lai entre requÃªtes
export SCRAPER_DELAY=2
```

## ğŸ›¡ï¸ Bonnes pratiques

âœ… **Respecter les serveurs**
- DÃ©lais alÃ©atoires entre requÃªtes (0.5-2s)
- User-Agent appropriÃ©
- Respect de `robots.txt`
- Rate limiting adaptatif

âœ… **Gestion d'erreurs robuste**
- Try-except complets
- Retry avec backoff exponentiel
- Timeout adaptatifs
- Logging dÃ©taillÃ©

âœ… **Code maintenable**
- Architecture orientÃ©e objet
- Docstrings complÃ¨tes
- Type hints (typing)
- SÃ©paration des prÃ©occupations

## ğŸ“ˆ Performance

### MÃ©triques attendues

Total pour les 9 exercices: **~15-20 minutes**

DÃ©tails par exercice voir [LIMITATIONS_QUANTITATIVES.md](LIMITATIONS_QUANTITATIVES.md)

### Optimisations

- Session pooling (connexions rÃ©utilisÃ©es)
- Cache pour Ã©viter requÃªtes dupliquÃ©es
- Parallel processing (Exercice 8)
- Checkpoints pour reprise rapide

## ğŸ§ª Tests

```bash
# Test rapide d'un exercice
python exercice_03_fake_jobs.py --keyword Python --max 10

# Test avec limite rÃ©duite
python exercice_01_books_scraper.py  # Modifier max_pages dans le code
```

## ğŸ“š DÃ©pendances principales

- **requests** : RequÃªtes HTTP
- **beautifulsoup4** : Parsing HTML
- **lxml** : Parser rapide
- **pandas** : Manipulation donnÃ©es
- **matplotlib** : Visualisations
- **networkx** : Graphes
- **pyyaml** : Configuration

Voir [requirements.txt](requirements.txt) pour versions complÃ¨tes.

## ğŸ¤ Contribution

Projet Ã©ducatif - Master 1 Algo 2025

## âš ï¸ Avertissement

Ces exercices utilisent des sites web d'entraÃ®nement publics:
- https://books.toscrape.com/
- http://quotes.toscrape.com/
- https://realpython.github.io/fake-jobs/

**Important**: Pour scraper des sites en production:
1. VÃ©rifier les conditions d'utilisation
2. Respecter `robots.txt`
3. Obtenir les autorisations nÃ©cessaires
4. Limiter la frÃ©quence des requÃªtes

## ğŸ“„ Licence

Code Ã©ducatif - Utilisation libre dans cadre acadÃ©mique

Â© 2025 - Tous droits rÃ©servÃ©s

## ğŸ”— Ressources

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/)
- [Requests Documentation](https://requests.readthedocs.io/)
- [Web Scraping Best Practices](https://github.com/topics/web-scraping)

---

**Auteur**: schwarzT404  
**Repository**: https://github.com/schwarzT404/scrapping
